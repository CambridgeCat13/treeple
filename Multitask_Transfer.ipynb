{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Efficiency Testing with SPORF, MORF and Honest Forest\n",
        "Here we assess transfer learning efficacy of SPORF, MORF and Honest Forest model over MNIST ➔ Fashion-MNIST tasks. We concentrate on the very-small target training sizes (0.1%–1%) to identify when transfer learning works because we aiming to show that the multitask classifier can harness the source domain data to improve performance in the severe data-limiting condition of the target domain. The second objective here is to know when transfer learning significantly increases the accuracy of the predictions and in the process to give evidence that the multitask classifier is functioning."
      ],
      "metadata": {
        "id": "I1_-497pKKdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Package Loading"
      ],
      "metadata": {
        "id": "Kjj65B28_d0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLTMD6wv9AGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2deca57c-7ae4-4b69-d19d-a88e3757dfa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/neurodata/treeple.git\n",
            "  Cloning https://github.com/neurodata/treeple.git to /tmp/pip-req-build-f2iz8ff4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neurodata/treeple.git /tmp/pip-req-build-f2iz8ff4\n",
            "  Resolved https://github.com/neurodata/treeple.git to commit 75c2cf919939574e4240fe261f053162039495cf\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from treeple==0.10.3) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from treeple==0.10.3) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from treeple==0.10.3) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->treeple==0.10.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->treeple==0.10.3) (3.6.0)\n",
            "Building wheels for collected packages: treeple\n",
            "  Building wheel for treeple (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treeple: filename=treeple-0.10.3-cp311-cp311-linux_x86_64.whl size=2924539 sha256=3f2a0430876d0d841db7f5497ce435e5483dcfac5f87360e7ac3bc0b01e915a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qyphpzr4/wheels/87/bd/56/9c9982c5af1eb667f0421e0000d3ed4b973b71c8e8903fa6cd\n",
            "Successfully built treeple\n",
            "Installing collected packages: treeple\n",
            "Successfully installed treeple-0.10.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/neurodata/treeple.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install proglearn"
      ],
      "metadata": {
        "id": "WwWV20EI9n_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad5cec7-177c-4f10-910a-0a583ba40125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting proglearn\n",
            "  Downloading proglearn-0.0.7-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: tensorflow>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from proglearn) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from proglearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from proglearn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from proglearn) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.11/dist-packages (from proglearn) (2.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->proglearn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.19.0->proglearn) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.19.0->proglearn) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.19.0->proglearn) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.19.0->proglearn) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.19.0->proglearn) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=1.19.0->proglearn) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.1.2)\n",
            "Downloading proglearn-0.0.7-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: proglearn\n",
            "Successfully installed proglearn-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from treeple import ObliqueRandomForestClassifier, PatchObliqueRandomForestClassifier, HonestForestClassifier\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ZSU8vg6m9sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multitask Classfier"
      ],
      "metadata": {
        "id": "5VMRJzdb_i9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MultiTaskForestClassifier:\n",
        "A unified multi-task learning wrapper for SPORF, MORF, and HonestForest.\n",
        "Trains on all tasks jointly and evaluates per-task performance.\n",
        "\"\"\"\n",
        "\n",
        "class MultiTaskForestClassifier:\n",
        "    def __init__(self, clf_type=\"SPORF\", task_ratios=None, random_state=42, **kwargs):\n",
        "        if clf_type == \"SPORF\":\n",
        "            self.model_cls = ObliqueRandomForestClassifier\n",
        "            self.default_params = {\n",
        "                \"n_estimators\": 200,\n",
        "                \"feature_combinations\": 2.0,\n",
        "                \"max_depth\": 20,\n",
        "                \"min_samples_split\": 5,\n",
        "                \"min_samples_leaf\": 1,\n",
        "                \"max_features\": 0.5,\n",
        "                \"bootstrap\": True\n",
        "            }\n",
        "        elif clf_type == \"MORF\":\n",
        "            self.model_cls = PatchObliqueRandomForestClassifier\n",
        "            self.default_params = {\n",
        "                \"n_estimators\": 200\n",
        "             }\n",
        "        elif clf_type == \"HonestForest\":\n",
        "            self.model_cls = HonestForestClassifier\n",
        "            self.default_params = {\n",
        "\n",
        "                \"n_estimators\": 200,\n",
        "                \"max_depth\": None,\n",
        "                \"min_samples_split\": 2,\n",
        "                \"min_samples_leaf\": 5,\n",
        "                \"bootstrap\": True,\n",
        "                \"max_features\": \"sqrt\"\n",
        "             }\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported tree: {clf_type}\")\n",
        "\n",
        "        self.params = {**self.default_params, **kwargs}\n",
        "        self.model = None\n",
        "        self.task_data = {}\n",
        "\n",
        "        if task_ratios is None:\n",
        "            warnings.warn(\"No task_ratios provided. Using default {0: 0.5, 1: 0.5}. Override if needed.\")\n",
        "            self.task_ratios = {0: 0.5, 1: 0.5}\n",
        "        else:\n",
        "            self.task_ratios = task_ratios\n",
        "\n",
        "\n",
        "        self.random_state = random_state\n",
        "\n",
        "\n",
        "    def add_task(self, task_id, X, y, test_size=0.2):\n",
        "        if test_size > 0:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y,\n",
        "                test_size=test_size,\n",
        "                stratify=y,\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "        else:\n",
        "            X_train, y_train = X, y\n",
        "            X_test, y_test = None, None\n",
        "\n",
        "        self.task_data[task_id] = {\n",
        "            \"train\": (X_train, y_train),\n",
        "            \"test\": (X_test, y_test)\n",
        "        }\n",
        "\n",
        "\n",
        "    def get_task_ids(self):\n",
        "      return list(self.task_data.keys())\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Train on all tasks jointly (multi-task learning).\"\"\"\n",
        "        X_all, y_all, task_labels = [], [], []\n",
        "        for task_id, data in self.task_data.items():\n",
        "            X, y = data[\"train\"]\n",
        "            X_all.append(X)\n",
        "            y_all.append(y)\n",
        "            task_labels.append(np.full(len(y), task_id))\n",
        "\n",
        "        X_all = np.vstack(X_all)\n",
        "        y_all = np.concatenate(y_all)\n",
        "        task_labels = np.concatenate(task_labels)\n",
        "        X_all = np.column_stack((X_all, task_labels))\n",
        "\n",
        "        self.model = self.model_cls(**self.params, random_state=42)\n",
        "        self.model.fit(X_all, y_all)\n",
        "\n",
        "    def predict(self, X, task_id):\n",
        "        \"\"\"Make predictions for a specific task.\"\"\"\n",
        "        X_task = np.column_stack((X, np.full(len(X), task_id)))\n",
        "        return self.model.predict(X_task)\n",
        "\n",
        "\n",
        "    def score(self, task_id):\n",
        "        \"\"\"Return accuracy on the held-out test set for a specific task.\"\"\"\n",
        "        X_test, y_test = self.task_data[task_id][\"test\"]\n",
        "        return accuracy_score(y_test, self.predict(X_test, task_id))\n",
        "\n",
        "def evaluate_transfer_efficiency(\n",
        "    X_source, y_source,\n",
        "    X_target, y_target,\n",
        "    target_ratios=[0.01],\n",
        "    seed=42,\n",
        "    clf_type=\"SPORF\"  #\"SPORF\", \"MORF\", \"HonestForest\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate transfer efficiency using the specified classifier type.\n",
        "    Fixes train/test split (80/20), and varies how much target training data is used.\n",
        "    \"\"\"\n",
        "    assert clf_type in [\"SPORF\", \"MORF\", \"HonestForest\"], f\"Invalid clf_type: {clf_type}\"\n",
        "    print(f\"\\n=== Transfer Efficiency using {clf_type} ===\")\n",
        "    X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(\n",
        "        X_source, y_source, test_size=0.2, stratify=y_source, random_state=seed\n",
        "    )\n",
        "    X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(\n",
        "        X_target, y_target, test_size=0.2, stratify=y_target, random_state=seed\n",
        "    )\n",
        "\n",
        "    for ratio in target_ratios:\n",
        "        if ratio < 1.0:\n",
        "            X_train_target_sub, _, y_train_target_sub, _ = train_test_split(\n",
        "                X_train_target, y_train_target,\n",
        "                train_size=ratio,\n",
        "                stratify=y_train_target,\n",
        "                random_state=seed\n",
        "            )\n",
        "        else:\n",
        "            X_train_target_sub, y_train_target_sub = X_train_target, y_train_target\n",
        "        clf_base = MultiTaskForestClassifier(\n",
        "            clf_type=clf_type,\n",
        "            task_ratios={1: 1.0},\n",
        "            random_state=seed\n",
        "        )\n",
        "        clf_base.add_task(1, X_train_target_sub, y_train_target_sub, test_size=0)\n",
        "        clf_base.fit()\n",
        "        acc_base = accuracy_score(y_test_target, clf_base.predict(X_test_target, task_id=1))\n",
        "        clf_transfer = MultiTaskForestClassifier(\n",
        "            clf_type=clf_type,\n",
        "            task_ratios={0: 1.0, 1: 1.0},\n",
        "            random_state=seed\n",
        "        )\n",
        "        clf_transfer.add_task(0, X_train_source, y_train_source, test_size=0)\n",
        "        clf_transfer.add_task(1, X_train_target_sub, y_train_target_sub, test_size=0)\n",
        "        clf_transfer.fit()\n",
        "        acc_transfer = accuracy_score(y_test_target, clf_transfer.predict(X_test_target, task_id=1))\n",
        "        num_samples = len(X_train_target_sub)\n",
        "        print(f\"Target Task Ratio: {ratio:.4f} ({num_samples} samples)\")\n",
        "        print(f\"  Baseline (Source 0%):   Accuracy = {acc_base:.3f}\")\n",
        "        print(f\"  Transfer (Source 100%): Accuracy = {acc_transfer:.3f}\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "hEhVvLA69CO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SPORF using the Multitask Clf\n",
        "**Task:** MNIST (classes 0 and 1) → FashionMNIST (classes 0 and 1) transfer\n",
        "\n",
        "**Model:** SPORF (Oblique Random Forest Classifier)\n",
        "\n",
        "**Goal:**  \n",
        "- Test transfer efficiency at extremely small target data ratios: 0.1%, 0.2%, 0.5%, 1%.  \n",
        "- Compare baseline (no transfer) vs. full transfer (source 100%).  \n",
        "- Report both **target ratio** and **number of samples** used.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZWX_r9il9fwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()\n",
        "(X_fashion_train, y_fashion_train), (X_fashion_test, y_fashion_test) = fashion_mnist.load_data()\n",
        "X_mnist = np.concatenate([X_mnist_train, X_mnist_test]).reshape(-1, 28*28) / 255.0\n",
        "y_mnist = np.concatenate([y_mnist_train, y_mnist_test])\n",
        "X_fashion = np.concatenate([X_fashion_train, X_fashion_test]).reshape(-1, 28*28) / 255.0\n",
        "y_fashion = np.concatenate([y_fashion_train, y_fashion_test])\n",
        "chosen_classes = [0, 1]\n",
        "\n",
        "mnist_mask = np.isin(y_mnist, chosen_classes)\n",
        "fashion_mask = np.isin(y_fashion, chosen_classes)\n",
        "\n",
        "X_mnist = X_mnist[mnist_mask]\n",
        "y_mnist = y_mnist[mnist_mask]\n",
        "X_fashion = X_fashion[fashion_mask]\n",
        "y_fashion = y_fashion[fashion_mask]\n",
        "y_mnist = np.where(y_mnist == chosen_classes[0], 0, 1)\n",
        "y_fashion = np.where(y_fashion == chosen_classes[0], 0, 1)\n",
        "\n",
        "subset = 10000\n",
        "X_mnist, y_mnist = X_mnist[:subset], y_mnist[:subset]\n",
        "X_fashion, y_fashion = X_fashion[:subset], y_fashion[:subset]\n",
        "\n",
        "evaluate_transfer_efficiency(\n",
        "    X_source=X_mnist,\n",
        "    y_source=y_mnist,\n",
        "    X_target=X_fashion,\n",
        "    y_target=y_fashion,\n",
        "    target_ratios=[0.001, 0.002, 0.005, 0.01],\n",
        "    seed=42,\n",
        "    clf_type=\"SPORF\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqBTcTOU9dLq",
        "outputId": "01a73f69-a007-4b71-b6b3-ee98c43a0969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Transfer Efficiency using SPORF ===\n",
            "Target Task Ratio: 0.0010 (8 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.918\n",
            "  Transfer (Source 100%): Accuracy = 0.909\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0020 (16 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.874\n",
            "  Transfer (Source 100%): Accuracy = 0.940\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0050 (40 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.967\n",
            "  Transfer (Source 100%): Accuracy = 0.961\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0100 (80 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.980\n",
            "  Transfer (Source 100%): Accuracy = 0.971\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results: Transfer Efficiency using SPORF (MNIST ➔ Fashion-MNIST)\n",
        "\n",
        "| Target Ratio | # Samples | Baseline Accuracy (Source 0%) | Transfer Accuracy (Source 100%) | Interpretation |\n",
        "|:------------:|:---------:|:-----------------------------:|:-------------------------------:|:--------------:|\n",
        "| 0.1%         | 8         | 0.918                         | 0.909                           | Noise (not meaningful) |\n",
        "| 0.2%         | 16        | 0.874                         | 0.940                           | Successful Transfer |\n",
        "| 0.5%         | 40        | 0.967                         | 0.961                           | Noise (small fluctuation) |\n",
        "| 1.0%         | 80        | 0.980                         | 0.971                           | Noise (small fluctuation) |\n",
        "\n",
        "---\n",
        "\n",
        "###Summary:\n",
        "\n",
        "- At **0.2%** (16 samples), **transfer learning improves accuracy** compared to baseline.\n",
        "- Other ratios (0.1%, 0.5%, 1.0%) show minor variations, but no clear benefit — mostly **noise**.\n",
        "- **Conclusion**: Transfer learning using SPORF is effective when training data is extremely limited."
      ],
      "metadata": {
        "id": "c7BJr5xuJB96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MORF using the Multitask Clf\n",
        "**Task:** MNIST (classes 0 and 1) → FashionMNIST (classes 0 and 1) transfer\n",
        "\n",
        "**Model:** MORF (Patch Oblique Random Forest Classifier)\n",
        "\n",
        "**Goal:**  \n",
        "- Test transfer efficiency at extremely small target data ratios: 0.1%, 0.2%, 0.5%, 1%.  \n",
        "- Compare baseline (no transfer) vs. full transfer (source 100%).  \n",
        "- Report both **target ratio** and **number of samples** used.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1mdTyiG_Ssxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()\n",
        "(X_fashion_train, y_fashion_train), (X_fashion_test, y_fashion_test) = fashion_mnist.load_data()\n",
        "X_mnist = np.concatenate([X_mnist_train, X_mnist_test]).reshape(-1, 28*28) / 255.0\n",
        "y_mnist = np.concatenate([y_mnist_train, y_mnist_test])\n",
        "X_fashion = np.concatenate([X_fashion_train, X_fashion_test]).reshape(-1, 28*28) / 255.0\n",
        "y_fashion = np.concatenate([y_fashion_train, y_fashion_test])\n",
        "chosen_classes = [0, 1]\n",
        "\n",
        "mnist_mask = np.isin(y_mnist, chosen_classes)\n",
        "fashion_mask = np.isin(y_fashion, chosen_classes)\n",
        "\n",
        "X_mnist = X_mnist[mnist_mask]\n",
        "y_mnist = y_mnist[mnist_mask]\n",
        "X_fashion = X_fashion[fashion_mask]\n",
        "y_fashion = y_fashion[fashion_mask]\n",
        "y_mnist = np.where(y_mnist == chosen_classes[0], 0, 1)\n",
        "y_fashion = np.where(y_fashion == chosen_classes[0], 0, 1)\n",
        "\n",
        "subset = 10000\n",
        "X_mnist, y_mnist = X_mnist[:subset], y_mnist[:subset]\n",
        "X_fashion, y_fashion = X_fashion[:subset], y_fashion[:subset]\n",
        "\n",
        "evaluate_transfer_efficiency(\n",
        "    X_source=X_mnist,\n",
        "    y_source=y_mnist,\n",
        "    X_target=X_fashion,\n",
        "    y_target=y_fashion,\n",
        "    target_ratios=[0.001, 0.002, 0.005, 0.01],\n",
        "    seed=42,\n",
        "    clf_type=\"MORF\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSCIrb9US_xI",
        "outputId": "945706b3-9e4b-4687-b5cf-47a8853eeab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "=== Transfer Efficiency using MORF ===\n",
            "Target Task Ratio: 0.0010 (8 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.651\n",
            "  Transfer (Source 100%): Accuracy = 0.648\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0020 (16 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.648\n",
            "  Transfer (Source 100%): Accuracy = 0.766\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0050 (40 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.681\n",
            "  Transfer (Source 100%): Accuracy = 0.786\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0100 (80 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.865\n",
            "  Transfer (Source 100%): Accuracy = 0.878\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results: Transfer Efficiency using MORF (MNIST ➔ Fashion-MNIST)\n",
        "\n",
        "| Target Ratio | # Samples | Baseline Accuracy (Source 0%) | Transfer Accuracy (Source 100%) | Interpretation |\n",
        "|:------------:|:---------:|:-----------------------------:|:-------------------------------:|:--------------:|\n",
        "| 0.1%         | 8         | 0.651                         | 0.648                           | Noise (small fluctuation) |\n",
        "| 0.2%         | 16        |                 0.648         |             0.766               | Successful Transfer |\n",
        "| 0.5%         | 40        |  0.681                        |   0.786                         | Successful Transfer  |\n",
        "| 1.0%         | 80        |          0.865                |   0.878                         | Successful Transfer  |\n",
        "\n",
        "---\n",
        "\n",
        "###Summary:\n",
        "\n",
        "- At **0.2%**, **0.5%** and **1.0%** (16, 40 and 80 samples), **transfer learning improves accuracy** compared to baseline.\n",
        "- 0.1% ratio shows a minor variation, but no clear benefit — mostly **noise**.\n",
        "- **Conclusion**: Transfer learning using MORF is effective when training data is extremely limited."
      ],
      "metadata": {
        "id": "KWfxfD-WTLe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HONEST FOREST"
      ],
      "metadata": {
        "id": "uGuuMDaq5YBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()\n",
        "(X_fashion_train, y_fashion_train), (X_fashion_test, y_fashion_test) = fashion_mnist.load_data()\n",
        "X_mnist = np.concatenate([X_mnist_train, X_mnist_test]).reshape(-1, 28*28) / 255.0\n",
        "y_mnist = np.concatenate([y_mnist_train, y_mnist_test])\n",
        "X_fashion = np.concatenate([X_fashion_train, X_fashion_test]).reshape(-1, 28*28) / 255.0\n",
        "y_fashion = np.concatenate([y_fashion_train, y_fashion_test])\n",
        "chosen_classes = [0, 1]\n",
        "\n",
        "mnist_mask = np.isin(y_mnist, chosen_classes)\n",
        "fashion_mask = np.isin(y_fashion, chosen_classes)\n",
        "\n",
        "X_mnist = X_mnist[mnist_mask]\n",
        "y_mnist = y_mnist[mnist_mask]\n",
        "X_fashion = X_fashion[fashion_mask]\n",
        "y_fashion = y_fashion[fashion_mask]\n",
        "y_mnist = np.where(y_mnist == chosen_classes[0], 0, 1)\n",
        "y_fashion = np.where(y_fashion == chosen_classes[0], 0, 1)\n",
        "\n",
        "subset = 10000\n",
        "X_mnist, y_mnist = X_mnist[:subset], y_mnist[:subset]\n",
        "X_fashion, y_fashion = X_fashion[:subset], y_fashion[:subset]\n",
        "\n",
        "evaluate_transfer_efficiency(\n",
        "    X_source=X_mnist,\n",
        "    y_source=y_mnist,\n",
        "    X_target=X_fashion,\n",
        "    y_target=y_fashion,\n",
        "    target_ratios=[0.002, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5],\n",
        "    seed=42,\n",
        "    clf_type=\"HonestForest\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id30ZOu-5meJ",
        "outputId": "cd8343e1-3a12-481d-87e8-1d64f95be9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Transfer Efficiency using HonestForest ===\n",
            "Target Task Ratio: 0.0020 (16 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.498\n",
            "  Transfer (Source 100%): Accuracy = 0.782\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0050 (40 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.947\n",
            "  Transfer (Source 100%): Accuracy = 0.810\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0100 (80 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.972\n",
            "  Transfer (Source 100%): Accuracy = 0.863\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0200 (160 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.978\n",
            "  Transfer (Source 100%): Accuracy = 0.895\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0300 (240 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.979\n",
            "  Transfer (Source 100%): Accuracy = 0.910\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0400 (320 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.981\n",
            "  Transfer (Source 100%): Accuracy = 0.925\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.0500 (400 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.980\n",
            "  Transfer (Source 100%): Accuracy = 0.925\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.1000 (800 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.978\n",
            "  Transfer (Source 100%): Accuracy = 0.949\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.2000 (1600 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.981\n",
            "  Transfer (Source 100%): Accuracy = 0.959\n",
            "--------------------------------------------------\n",
            "Target Task Ratio: 0.5000 (4000 samples)\n",
            "  Baseline (Source 0%):   Accuracy = 0.983\n",
            "  Transfer (Source 100%): Accuracy = 0.972\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}