import numpy as np
cimport numpy as cnp
cnp.import_array()

cdef class UnsupervisedCriterion(BaseCriterion):
    """Abstract criterion for unsupervised learning.
    
    This object is a copy of the Criterion class of scikit-learn, but is used
    for unsupervised learning. However, ``Criterion`` in scikit-learn was
    designed for supervised learning, where the necessary
    ingredients to compute a split point is solely with y-labels. In
    this object, we subclass and instead rely on the X-data.    

    This object stores methods on how to calculate how good a split is using
    different metrics for unsupervised splitting.
    """
    def __cinit__(self):
        """Initialize attributes for this criterion.

        Parameters
        ----------
        X : array-like, dtype=DTYPE_t
            The dataset stored as a buffer for memory efficiency of shape
            (n_samples,).
        """
        self.start = 0
        self.pos = 0
        self.end = 0

        self.n_samples = 0
        self.n_node_samples = 0
        self.weighted_n_node_samples = 0.0
        self.weighted_n_left = 0.0
        self.weighted_n_right = 0.0

        # Initialize count metric for current, going left and going right
        self.sum_total = 0.0
        self.sum_left = 0.0
        self.sum_right = 0.0

    cdef int init(
        self,
        const DOUBLE_t[:,:] X,
        const DOUBLE_t[:] sample_weight,
        double weighted_n_samples,
        const SIZE_t[:] sample_indices
    ) nogil except -1:
        """Initialize the criterion.

        This initializes the criterion at node samples[start:end] and children
        samples[start:start] and samples[start:end].

        Returns -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.

        Parameters
        ----------
        X : array-like, dtype=DOUBLE_t
            The data-feature matrix stored as a buffer for memory efficiency. Note that
            this is not used, but simply passed as a convenience function.
        sample_weight : array-like, dtype=DOUBLE_t
            The weight of each sample (i.e. row of X).
        weighted_n_samples : double
            The total weight of all samples.
        samples : array-like, dtype=SIZE_t
            A mask on the samples, showing which ones we want to use
        """
        self.sample_weight = sample_weight
        self.samples = samples
        self.start = start
        self.end = end
        self.n_node_samples = end - start
        self.weighted_n_samples = weighted_n_samples
        self.weighted_n_node_samples = 0.0

        cdef SIZE_t i
        cdef SIZE_t p
        cdef DOUBLE_t w = 1.0
        self.sum_total = 0.0

        for p in range(start, end):
            i = samples[p]

            # w is originally set to be 1.0, meaning that if no sample weights
            # are given, the default weight of each sample is 1.0
            if sample_weight != NULL:
                w = sample_weight[i]
            
            # ith sample; overall compute the weighted average
            self.sum_total += w * X[i]
            self.weighted_n_node_samples += w

        # Reset to pos=start
        self.reset()
        return 0


cdef class TwoMeans(UnsupervisedCriterion):
    r"""Two means split impurity.

    The two means split finds the cutpoint that minimizes the one-dimensional
    2-means objective, which is finding the cutoff point where the total variance
    from cluster 1 and cluster 2 are minimal. 

    The mathematical optimization problem is to find the cutoff index ``s``,
    which is called 'pos' in scikit-learn.

        \min_s \sum_{i=1}^s (x_i - \hat{\mu}_1)^2 + \sum_{i=s+1}^N (x_i - \hat{\mu}_2)^2

    where x is a N-dimensional feature vector, N is the number of samples and the \mu
    terms are the estimated means of each cluster 1 and 2.

    Node-Wise Feature Generation
    ----------------------------
    URerF doesn't choose split points in the original feature space
    It follows the random projection framework

    \tilde{X}= A^T X'

    where, A is p x d matrix distributed as f_A, where f_A is the 
    projection distribution and d is the dimensionality of the 
    projected space. A is generated by randomly sampling from 
    {-1,+1} lpd times, then distributing these values uniformly 
    at random in A. l parameter is used to control the sparsity of A
    and is set to 1/20.

    Each of the d rows \tilde{X}[i; :], i \in {1,2,...d} is then 
    inspected for the best split point. The optimal split point and 
    splitting dimension are chosen according to which point/dimension
    pair minimizes the splitting criteria described in the following 
    section
    """

    cdef __cinit__(self, SIZE_t n_outputs):
        """Initialize parameters for this criterion.

        Parameters
        ----------
        n_outputs : SIZE_t
            The number of targets to be predicted
        """
        #TODO: maybe pass user specified number of clusters?
        self.n_outputs = n_outputs

    cdef int init(
        self,
        const DTYPE_t[:, :] X,
        const DOUBLE_t[:] sample_weight,
        double weighted_n_samples,
        const SIZE_t[:] sample_indices
    ) nogil except -1:
        """Initialize the criterion.

        This initializes the criterion at node samples[start:end] and children
        samples[start:start] and samples[start:end].

        Returns -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.

        Parameters
        ----------
        X : array-like, dtype=DOUBLE_t
            The target stored as a buffer for memory efficiency. Note that
            this is not used, but simply passed as a convenience function.
        sample_weight : array-like, dtype=DOUBLE_t
            The weight of each sample (i.e. row of X).
        weighted_n_samples : double
            The total weight of all samples.
        samples : array-like, dtype=SIZE_t
            A mask on the samples, showing which ones we want to use
        """
        self.X = X
        self.sample_weight = sample_weight
        self.weighted_n_samples = weighted_n_samples
        self.sample_indices = sample_indices

        return 0
        
    cdef int reset(self) nogil except -1:
        """Reset the criterion at pos=start.

        Returns -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.
        """
        self.pos = self.start

        self.weighted_n_left = 0.0
        self.weighted_n_right = self.weighted_n_node_samples
        cdef SIZE_t k

        # TODO: Again UnsupervisedCriterion has no notion of "classes", so irrelevant here.
        # However, you need to reset sum_left and sum_right right?
        for k in range(self.n_outputs):
            memset(&self.sum_left[k], 0, sizeof(double))
            memcpy(&self.sum_right[k], &self.sum_total[k], sizeof(double))
        return 0

    cdef int reverse_reset(self) nogil except -1:
        """Reset the criterion at pos=end.

        Returns -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.
        """
        self.pos = self.end

        self.weighted_n_left = self.weighted_n_node_samples
        self.weighted_n_right = 0.0
        cdef SIZE_t k

        # TODO: Again UnsupervisedCriterion has no notion of "classes", so irrelevant here.
        # However, you need to reset sum_left and sum_right right?
        for k in range(self.n_outputs):
            memset(&self.sum_right[k], 0, sizeof(double))
            memcpy(&self.sum_left[k], &self.sum_total[k], sizeof(double))
        return 0

    cdef int update(self, SIZE_t new_pos) nogil except -1:
        """Updated statistics by moving samples[pos:new_pos] to the left child.

        Returns -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.

        Parameters
        ----------
        new_pos : SIZE_t
            The new ending position for which to move samples from the right
            child to the left child.
        """
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end

        cdef const SIZE_t[:] sample_indices = self.sample_indices
        cdef const DOUBLE_t[:] sample_weight = self.sample_weight

        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef DOUBLE_t w = 1.0

        # Update statistics up to new_pos
        #
        # Given that
        #   sum_left[x] +  sum_right[x] = sum_total[x]
        # and that sum_total is known, we are going to update
        # sum_left from the direction that require the least amount
        # of computations, i.e. from pos to new_pos or from end to new_po.
        if (new_pos - pos) <= (end - new_pos):
            for p in range(pos, new_pos):
                i = sample_indices[p]

                if sample_weight is not None:
                    w = sample_weight[i]

                # TODO: sum_left updated
                for k in range(self.n_outputs):
                    self.sum_left[k] += w * self.X[i]

                self.weighted_n_left += w
        else:
            self.reverse_reset()

            for p in range(end - 1, new_pos - 1, -1):
                i = sample_indices[p]

                if sample_weight is not None:
                    w = sample_weight[i]

                # TODO: sum_left updated
                for k in range(self.n_outputs):
                    self.sum_left[k] -= w * self.X[i]

                self.weighted_n_left -= w

        # Update right part statistics
        self.weighted_n_right = (self.weighted_n_node_samples - 
                                self.weighted_n_left)
        # TODO: no notion of classes
        for k in range(self.n_outputs):
            self.sum_right[k] = self.sum_total[k] - self.sum_left[k]

        self.pos = new_pos
        return 0

    cdef double node_impurity(self) nogil:
        """Evaluate the impurity of the current node.

        Evaluate the TwoMeans criterion as impurity of the current node,
        i.e. the impurity of sample_indices[start:end]. The smaller the impurity the
        better.

        TODO:
        Review on TwoMeans specific methods: Good start, and it's def in the right direction.
        impurity in TwoMeans is defined as the Variance. 
        
        Variance formula would be 
        impurity = weight * (left_impurity + right_impurity) / n_samples 
        and then left_impurity and right_impurity are defined as: 
        weight * Variance of left child and weight * Variance of right child. 

        So you need to compute variance in node_impurity and children_impurity. 
        The only thing changing are the pointers.

        You should therefore define a separate function to compute variance.
        Focus on getting the Criterion correct and ping me w/ questions. We can move onto the splitter once this is done.
        """

        cdef double impurity = 0.0

        # TODO: impurity in TwoMeans criterion is the variance.
        # variance = weight * (left_impurity + right_impurity) / n_samples
        impurity =  self._compute_variance() / self.weighted_n_node_samples

        return impurity

    cdef void _compute_variance(self) nogil:
        """Computes variance of left and right cluster at `pos`
        """
        cdef SIZE_t pos = self.pos
        cdef SIZE_t end = self.end
        cdef SIZE_t i
        cdef SIZE_t p
        cdef DOUBLE_t w
        cdef double left_c = 0.0 #left cluster
        cdef double right_c = 0.0 #right cluster
        cdef double mu_left #left mean
        cdef double mu_right #right mean

        cdef const DTYPE_t[:,:] X = self.X
        
        # TODO: these need to be redone 
        for p in range(end):
            i = self.sample_indices[p]

            if p < pos:
                mu_left += X[i] / self.n_samples
            else:
                mu_right += X[i] / self.n_samples

        for p in range(end):
            i = self.sample_indices[p]

            if self.sample_weight is not None:
                w = self.sample_weight[i]

            if k < pos:
                left_c += w * (X[i] - mu_left)**2
            else:
                right_c += w * (X[i] - mu_right)**2

        return (left_c + right_c) / self.n_samples

    cdef void children_impurity(self, double* impurity_left,
                                    double* impurity_right) nogil:
       """Evaluate the impurity in children nodes.

        i.e. the impurity of the left child (samples[start:pos]) and the
        impurity the right child (samples[pos:end]).

        Parameters
        ----------
        impurity_left : double pointer
            The memory address to save the impurity of the left node
        impurity_right : double pointer
            The memory address to save the impurity of the right node
        """
        # cdef double entropy_left = 0.0
        # cdef double entropy_right = 0.0
        # cdef double count_k
        # cdef SIZE_t k
        # cdef SIZE_t c

        # for k in range(self.n_outputs):
        #     for c in range(self.n_classes[k]):
        #         count_k = self.sum_left[k, c]
        #         if count_k > 0.0:
        #             count_k /= self.weighted_n_left
        #             entropy_left -= count_k * log(count_k)

        #         count_k = self.sum_right[k, c]
        #         if count_k > 0.0:
        #             count_k /= self.weighted_n_right
        #             entropy_right -= count_k * log(count_k)

        # impurity_left[0] = entropy_left / self.n_outputs
        # impurity_right[0] = entropy_right / self.n_outputs
        
        # use left_sum and right_sum to calculate impurity?
        # or should this method take two separate Xs making up
        # left and right child nodes?
        pass

    cdef void node_value(self, double* dest) nogil:
        r"""Compute the node value of samples[start:end] and save it into dest.

        TODO: No, it does not handle the computation. 
        See scikit-learn splitter and tree for how node_value() is used.

        Parameters
        ----------
        dest : double pointer
            The memory address which we will save the node value into.
        """
        cdef SIZE_t k

        # TODO: no notion of classes
        # Possible tip: See how node_value is used in splitter and tree.
        for k in range(self.n_outputs):
            memcpy(dest, &self.sum_total[k], sizeof(double))
            # TODO: need to revisit
            # dest += self.sum_total[k] / self.weighted_n_node_samples

    cdef void set_sample_pointers(
        self,
        SIZE_t start,
        SIZE_t end
    ) nogil:
        """Set sample pointers in the criterion."""
        self.n_node_samples = end - start
        self.start = start
        self.end = end

        self.weighted_n_node_samples = 0.0

        cdef SIZE_t i
        cdef SIZE_t p
        cdef SIZE_t k
        cdef DOUBLE_t w = 1.0

        memset(&self.sum_total[0], 0, sizeof(double))

        for p in range(start, end):
            i = self.sample_indices[p]

            # w is originally set to be 1.0, meaning that if no sample weights
            # are given, the default weight of each sample is 1.0.
            if self.sample_weight is not None:
                w = self.sample_weight[i]

            for k in range(self.n_outputs):
                X_i = self.X[i]
                w_X_i = w * X_i
                self.sum_total[k] += w_X_i
                self.sq_sum_total += w_X_i * X_i

            self.weighted_n_node_samples += w

        # Reset to pos=start
        self.reset()